\section{Conclusion}

\subsection{Summary of Contributions}

The main focus of this research is investigating the different strategies to accelerate Reinforcement Learning. The exploration phase of Reinforcement Learning takes a big chunk of the overall training time and becomes increasingly difficult with more complex environments. Two strategies are explored to overcome this exploration. First is the use of demonstrations and training using a supervised approach. In this method, Behaviour Cloning which is a simple form of imitation learning is combined with a Reinforcement Learning agent, incorporating demonstrations into the training to overcome this exploration phase and accelerate the learning. A new loss function (a combination of losses from previous research)
\cite{nair2018overcoming} \cite{goecks2020integrating} is introduced along with a new training strategy which includes a pre-training phase and a different way of presenting the data to the agent. The loss function is designed in such a way that the agent is not limited or constrained by the demo behaviours and is free to develop new behaviours and grow beyond the demo actions. The noise was added to the demonstrations and different sources of the demonstrations were tested to show that this method can cope with change and also with suboptimal demos and still perform well. The second strategy is the use of a simple and informative reward function to further support accelerated learning. The research was careful not to implement an over-engineered and complex reward function which might lead to suboptimal behaviour and has stuck to simplicity. The only drawback is the availability of demonstrations as this method relies on demos to work, but other than that if demos are available this research offers a simple and straightforward implementation compared to other researches in the same domain. The proposed method can perform much better than previous related baselines while striking a good balance between model complexity and convergence rate, also hardware complexity and training time, providing a middle ground in this complex domain. \\

\subsection{Summary of Results}
\begin{table}[h!]
\centering
\begin{tabular}{|c|cc|cc|c|}
\hline
Fetch Environment & Baseline I               &      & Current Implementation    &      & Speedup   \\ \hline
 & \multicolumn{1}{c|}{Timesteps} & Success Rate & \multicolumn{1}{c|}{Timesteps} & Success Rate &  \\ \hline
Reach             & \multicolumn{1}{c|}{150K} & 1.0  & \multicolumn{1}{c|}{16K}  & 1.0  & $\sim$9x  \\ \hline
Pick and Place    & \multicolumn{1}{c|}{10M} & 0.0  & \multicolumn{1}{c|}{310K} & 1.0  & -         \\ \hline
Push              & \multicolumn{1}{c|}{10M} & 0.85 & \multicolumn{1}{c|}{1.1M} & 0.95 & $\sim$10x \\ \hline
Slide             & \multicolumn{1}{c|}{10M} & 0.4  & \multicolumn{1}{c|}{2M}   & 0.6  & $\sim$5x  \\ \hline
\end{tabular}
\caption{Comparison of Current Implementation with Baseline 1}
\label{tab:my-table1}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{|c|cc|cc|c|}
\hline
Fetch Environment & Baseline II             &     & Current Implementation    &      & Speedup  \\ \hline
               & \multicolumn{1}{c|}{Timesteps} & Success Rate & \multicolumn{1}{c|}{Timesteps} & Success Rate &          \\ \hline
Reach             & \multicolumn{1}{c|}{-}  & -   & \multicolumn{1}{c|}{16K}  & 1.0  & -        \\ \hline
Pick and Place & \multicolumn{1}{c|}{1M}        & 0.9          & \multicolumn{1}{c|}{310K}      & 1.0          & $\sim$3x \\ \hline
Push              & \multicolumn{1}{c|}{3M} & 0.9 & \multicolumn{1}{c|}{1.1M} & 0.95 & $\sim$3x \\ \hline
Slide             & \multicolumn{1}{c|}{8M} & 0.6 & \multicolumn{1}{c|}{2M}   & 0.6  & $\sim$4x \\ \hline
\end{tabular}
\caption{Comparison of Current Implementation with Baseline 2}
\label{tab:my-table2}
\end{table}

\begin{table}[h!]
\centering
\begin{tabular}{|l|l|l|}
\hline
Fetch Environment & Baseline III   & Current Implementation \\ \hline
                  & Success Rate & Success Rate           \\ \hline
Reach             & -            & 1.0                    \\ \hline
Pick and Place    & 0.8          & 1.0                    \\ \hline
Push              & 0.2          & 0.95                   \\ \hline
Slide             & 0.2          & 0.6                    \\ \hline
\end{tabular}
\caption{Comparison of Current Implementation with Baseline 3}
\label{tab:my-table}
\end{table}

The highlights of the results for the current implementation are, for the same environment suite when compared to previous related results this implementation can provide a much better convergence rate showing accelerated learning. Baseline 1 was the first to solve these complex environments but with a high convergence rate. This was compensated by using high-end hardware resources in the form of distributed computing and also in the form of some environment-related tricks to solve the more complex tasks. Baseline 2 performs much better showcasing the advantages of the current implementation. Baseline 3 is more of a hit or miss, this is one of the main problems of pure Behaviour Cloning methods, either they can solve the task very well or struggle badly. This entirely depends upon the source, quality and quantity of the demonstrations. Also using only pure dense reward functions can solve the task but not provide a good accuracy. This is where the current implementation can outperform the baselines by combining Behaviour Cloning with Reinforcement Learning and dense reward functions. Also, the fetch push task was able to develop new behaviours which have both positive and negative outcomes. The negative is that even though the task is solved successfully this behaviour might be okay within a simulation but may not be a desired behaviour for the real world. The positive is that apart from solving the tasks successfully and providing accelerated learning, the current implementation can outperform the demos and also develop new behaviours. Apart from the straightforward implementation, the training was performed in using modest hardware resources on a laptop computer providing a middle ground in the form of a framework to solve complex tasks. \\

\subsection{Answering Research Questions}

\begin{itemize}[leftmargin=0.7in]
    \item[Q1.  ] \textbf{Can the proposed method successfully solve complex robotics tasks?} \\

    \item[A1.  ] The current implementation can successfully solve all the tasks in the complex multi-goal tasks in the fetch robotics environment suite with a success rate and faster convergence rate across all the tasks. \\

    \item[Q2.  ] \textbf{Does the proposed method achieve accelerated learning without a compromise to generalization or resulting in any undesirable outcomes?} \\
    
    \item[A2.  ] The current implementation can provide accelerated learning across all the environments without any loss in generalization. After training the current implementation was tested across 5 different random seed test environments and has good performance very close to that of the training results, except for the fetch slide environment which has the highest complexity of the lot, but these results were within margin when compared to the baselines. For three environments the resultant behaviour was as expected, but for the fetch push task even though the task was solved successfully, the resultant behaviour might be suitable for a simulation but not desirable for the real world. This is not a major cause for concern, the reason is explained in the results section and this problem can be solved as an extension to this research. \\
    
    \item[Q3.  ] \textbf{Does the source, quality and quantity of the demonstrations used matter for performance?} \\
    
    \item Two different sources of demonstrations were tested with the current implementation and the observation is that the sources of demonstrations do impact performance, but not in a way that the algorithm is unusable. The observations have been presented in the results section and the summary is that both the sources of demonstrations were able to outperform the baselines. The noise was added to the demonstrations and both success and failure cases were shown to the agent and the agent was able to learn and perform well from these suboptimal demonstrations. Finally compared to the baselines the current implementation uses a fewer number of demonstrations but this number depends on the complexity of the environment but is overall lesser than the baselines. \\

    \item[Q4.  ] \textbf{How does the proposed method compare to previous related baselines?} \\
    
    \item In terms of convergence rate the current implementation performs much better than the related baselines. Some of the methods compensate for this by either using complex implementations or high-end hardware resources reducing the training time. The current implementation can provide a balance between implementation simplicity and modest hardware resources used. The only drawback is there has to be some form of demonstrations available as this method is heavily reliant on the demos and will not work without them. \\
\end{itemize}

\subsection{Future Work}

As an extension to this research, it would be interesting to explore a few other aspects to either solve some existing condition or observe something new. The first and most important area is to address the different emergent behaviour for the fetch push environment. As mentioned earlier the task is solved successfully and it might be an okay result for the simulation but not a desirable action for the real world. This can be solved by either changing the soft-constrains dynamics of the environment or adding a term in the reward function to change the desired behaviour, these related areas can be explored. The second would be to improve performance in complex tasks like the fetch slide environment. This can be done by transitioning from Behaviour Cloning to Reinforcement Learning completely. The current method uses a fixed ratio of Behaviour Cloning and Reinforcement Learning losses, but it would be interesting to see this approach of first learning using Behaviour Cloning and then slowly transitioning to Reinforcement Learning and then using pure Reinforcement Learning to further continue the training to fine-tune the performance and learning. It would be interesting to see if there is any improvement in the performance and if any new behaviours are observed and in the slide environment if the max success rate threshold can be beaten. The third would be playing with the demonstrations, trying out different sources, also changing the quality and the number of demonstrations. A higher number of lower quality demonstrations or a lower number of higher quality demonstrations can be tried out and the performance can be studied. Finally, it would be interesting to extend this current implementation to other multi-goal environments, related to robotics like hand-manipulation, or sequential-goals related to robotics like stacking blocks, or non-robotics related tasks like walking, running, balancing, etc \cite{brockman2016openai}. \\
